{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Semesteroppgave"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Text preparation of test sets\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports for data preparation \n",
    "import os\n",
    "import os.path\n",
    "import re\n",
    "import json\n",
    "\n",
    "# imports for machine learning\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import ConfusionMatrixDisplay, classification_report, confusion_matrix\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "\n",
    "# Auxiliary\n",
    "import numpy as np\n",
    "from random import shuffle\n",
    "\n",
    "\n",
    "# sentiment analyse for nowegain, norec corpus\n",
    "# norec sentences , github\n",
    "# 123 neg, 56 pos\n",
    "# aruca finetuning sentiment\n",
    "# small norbert3 \n",
    "# kushtrin, sentiment \n",
    "# mlp classifier på topp\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Investageting the size of the directories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_dir(pn):\n",
    "    for folder in os.listdir(pn):\n",
    "        full_path = pn+folder\n",
    "        if os.path.isdir(full_path):\n",
    "            folder_size = sum(os.path.getsize(os.path.join(full_path, file)) for file in os.listdir(full_path))\n",
    "            folder_size_kb = round(folder_size / 1024, 2)      \n",
    "            num_files = sum(1 for f in os.listdir(\"./\"+full_path))\n",
    "\n",
    "        print(f\"{folder} size: {folder_size_kb} kB, {num_files} files\")\n",
    "    print(\"\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "VG contains: \n",
      "2005 size: 50307.78 kB, 264 files\n",
      "2006 size: 74925.64 kB, 360 files\n",
      "2007 size: 67835.89 kB, 351 files\n",
      "2008 size: 50588.86 kB, 359 files\n",
      "2009 size: 148471.12 kB, 352 files\n",
      "2010 size: 129299.64 kB, 359 files\n",
      "2011 size: 103358.62 kB, 359 files\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"VG contains: \")\n",
    "check_dir(\"./vg/\")\n",
    "\n",
    "# print(\"Nordlys contains: \")\n",
    "# check_dir(\"./vg/\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cleaning the test data and storing it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# text prepartion function\n",
    "\n",
    "def clean_text(text):\n",
    "    text = text.lower()\n",
    "    text = text.replace(\"¶\", \"\")\n",
    "    text = re.sub(r'<[^>]*>|^\\s*</[^>]*>$', '', text, flags=re.MULTILINE) # removing xml tags \n",
    "    text = re.sub(r'\\b\\w*_{1}\\w{10,}\\b', '', text, flags=re.MULTILINE)\n",
    "    text = re.sub(r'[^\\w\\s.,!?]', '', text, flags=re.MULTILINE) # removing non-word chars and whitespaces\n",
    "    text = re.sub(r'https?://(?:www\\.)?[\\w\\.-]+\\.\\w{2,}(?:/\\S*)?', '', text, flags=re.MULTILINE) # remove url\n",
    "    \n",
    "    return text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# \n",
    "\n",
    "def create_dir(dir_path):\n",
    "    pattern = re.compile(r\">(.*?)##\", re.DOTALL) # regex compile pattern, separates to article \n",
    "\n",
    "    for file_name in os.listdir(f\"{dir_path}\"): # iterate every filname in target dir\n",
    "        file_path = os.path.join(dir_path, file_name)\n",
    "        with open(file_path, \"r\", encoding=\"latin-1\") as infile: # read files\n",
    "            html_text = infile.read()\n",
    "            matches = pattern.findall(html_text) # separates every article based on regex pattern (from > to ##)\n",
    "            \n",
    "                        \n",
    "        file_wo_format = file_name[:-6] # file name without format\n",
    "        path_split = dir_path.split(\"/\") \n",
    "        new_dir_path = f\"./test_{path_split[-2]}/test_{path_split[-1]}\" \n",
    "        os.makedirs(new_dir_path,exist_ok=True)\n",
    "\n",
    "        with open(f\"./{new_dir_path}/{file_wo_format}.txt\", \"w\") as outfile: # cleans text further and writes to new format\n",
    "            for text in matches:\n",
    "                cleaned_text = clean_text(text.strip())\n",
    "                outfile.write(cleaned_text.strip()+\"\\n\")\n",
    "     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating test file directories for different news papers with years\n",
    "\n",
    "search_paths = [\"./vg\",] \n",
    "\n",
    "\n",
    "for paths in search_paths:\n",
    "    for path in os.listdir(paths):\n",
    "        create_dir(f\"./{paths}/{path}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Deleting articles that does not meet the requirements (elimination by search phrase)\n",
    "#### Assigning articles to synthetic ids\n",
    "#### Creating ajoining metadata in json file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function used for iterating through each new test folder\n",
    "# takes list as param to find kewords and rearrange articles\n",
    "\n",
    "def create_test_set(path, phrases):\n",
    "    metadata_dict = {} # dict for metadata file \n",
    "    pattern1 = re.compile(r'\\b(?:' + '|'.join(phrases) + r')\\b', re.IGNORECASE) # joins words with phrases that are relevant\n",
    "    exclude_words = {\"eksamen\", \"Eksamen\"} # excludes words that are completely irrelevant\n",
    "\n",
    "    for file_name in os.listdir(path):\n",
    "        file_path = path+\"/\"+file_name\n",
    "        split_path = file_path.split(\"/\")\n",
    "        id_name = file_name[:-4]\n",
    "\n",
    "        with open(file_path, \"r\", encoding=\"latin-1\") as file:\n",
    "            content = file.read()\n",
    "            articles = re.split(r'\\n\\s*\\n', content)\n",
    "\n",
    "        count = 1\n",
    "        for article in articles:\n",
    "            if re.search(pattern1, article):\n",
    "                if not any(exclude in article.lower() for exclude in exclude_words):\n",
    "                    out_file = f\"{id_name}-{count}\"\n",
    "                    out_file_path = f\"./test/{out_file}.txt\"\n",
    "                    metadata_dict[out_file] = {\"paper\":split_path[-3], \"year\":split_path[-2], \"sentiment_score\":None}\n",
    "                    with open(out_file_path, \"w\", encoding=\"latin-1\") as file:\n",
    "                        file.write(article)\n",
    "                    count += 1\n",
    "\n",
    "    return metadata_dict\n",
    "    # with open(\"test_metadata.json\", \"w\") as json_file:\n",
    "    #     json.dump(metadata_dict, json_file, indent=4)\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Directory created successfully.\n"
     ]
    }
   ],
   "source": [
    "# list of search paths from work work directory\n",
    "# including directory for vg, fedrelandsvennen, nordlys\n",
    "\n",
    "metadata_dict = {}\n",
    "search_phrases = [\"samers?\", \"samenes?\", \"samisk\", \"sápmi\", \"sametinget\", \"reindrift\", \"reinsdyr\", \"urbefolknings rettigheter\"]\n",
    "search_paths = [\"./test_vg\"]\n",
    "\n",
    "\n",
    "if not os.path.exists(\"./test\"):\n",
    "    os.mkdir(\"./test\")\n",
    "    print(\"Directory created successfully.\")\n",
    "else: \n",
    "    print(\"Directory already exists\")\n",
    "\n",
    "\n",
    "for paths in search_paths:\n",
    "    for path in os.listdir(paths):\n",
    "        meta_temp = create_test_set(f\"./{paths}/{path}\", search_phrases)\n",
    "        metadata_dict.update(meta_temp)\n",
    "\n",
    "with open(\"./test_metadata.json\", \"w\") as json_file:\n",
    "    json.dump(metadata_dict, json_file, indent=4)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Making sure metadata and num files are same length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "205\n"
     ]
    }
   ],
   "source": [
    "count = 0\n",
    "for files in os.listdir(\"./test\"):\n",
    "    count += 1\n",
    "print(count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "205\n"
     ]
    }
   ],
   "source": [
    "with open(\"test_metadata.json\", \"r\") as f:\n",
    "    d = json.load(f)\n",
    "print(len(d))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
